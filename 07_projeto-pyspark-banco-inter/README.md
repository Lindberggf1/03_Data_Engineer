
# Teste PySpark - Banco Inter

Este reposit√≥rio cont√©m a solu√ß√£o do **Teste de Conhecimentos em PySpark** proposto pelo Banco Inter.  
O objetivo foi aplicar conceitos de manipula√ß√£o de dados, fun√ß√µes avan√ßadas, performance, integra√ß√£o e processamento de grandes volumes de dados utilizando PySpark.

---

## Estrutura do Projeto

```
projeto-pyspark-banco-inter/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ parte1_manipulacao_dados.py          # Cria√ß√£o, sele√ß√£o, filtragem, agrupamento e ordena√ß√£o de DataFrame
‚îÇ   ‚îú‚îÄ‚îÄ parte2_funcoes_avancadas.py           # UDFs e Fun√ß√µes de Janela
‚îÇ   ‚îú‚îÄ‚îÄ parte3_performance_otimizacao.py      # Particionamento e Broadcast Join
‚îÇ   ‚îú‚îÄ‚îÄ parte4_integracao.py                  # Leitura de CSV, Escrita em Parquet, Integra√ß√£o com HDFS
‚îÇ   ‚îî‚îÄ‚îÄ parte5_problema_caso.py               # Processamento de Logs e An√°lise de Atividades
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/    # Arquivos CSV de entrada
‚îÇ   ‚îî‚îÄ‚îÄ output/   # Resultados gerados (Parquet e CSV)
‚îÇ
‚îî‚îÄ‚îÄ README.md     # Este arquivo
```

---

## Implementa√ß√£o

### Parte 1: Manipula√ß√£o de Dados
- Cria√ß√£o de DataFrame com colunas `Name`, `Age` e `Occupation`.
- Sele√ß√£o apenas das colunas `Name` e `Age`.
- Filtragem dos registros onde `Age` > 30 anos.
- Agrupamento por `Occupation` e c√°lculo da m√©dia de idade.
- Ordena√ß√£o dos resultados pela m√©dia de idade de forma decrescente.

### Parte 2: Fun√ß√µes Avan√ßadas
- Cria√ß√£o de uma **User Defined Function (UDF)** para classificar idade em:
  - Jovem (< 30)
  - Adulto (30-40)
  - S√™nior (> 40)
- Uso de **Window Functions** para calcular a diferen√ßa entre a idade do indiv√≠duo e a m√©dia de idade por ocupa√ß√£o.

### Parte 3: Performance e Otimiza√ß√£o
- **Particionamento** dos dados para otimizar leitura e escrita no armazenamento.
- **Broadcast Join** para otimizar jun√ß√µes de DataFrames onde um deles √© pequeno.

### Parte 4: Integra√ß√£o com Outras Tecnologias
- Leitura de arquivos CSV.
- Escrita de resultados no formato Parquet.
- Exemplo de integra√ß√£o e leitura/escrita em **Hadoop HDFS**.

### Parte 5: Problema de Caso - Processamento de Logs
- Carga de arquivo de log com `timestamp`, `user_id`, `action`.
- Contagem de n√∫mero de a√ß√µes realizadas por cada usu√°rio.
- Identifica√ß√£o dos 10 usu√°rios mais ativos.
- Escrita do resultado final em arquivo CSV.

---

## Como executar localmente

1. Instale as depend√™ncias:
    ```bash
    pip install pyspark
    ```

2. Inicie o ambiente PySpark (ou Databricks, ou ambiente Hadoop configurado).

3. Execute cada script da pasta `src/` de acordo com a parte do teste que deseja validar:
    ```bash
    python src/parte1_manipulacao_dados.py
    python src/parte2_funcoes_avancadas.py
    ...
    ```

---

## Boas Pr√°ticas Aplicadas

- **Modulariza√ß√£o:** Scripts separados por tema para organiza√ß√£o e melhor manuten√ß√£o.
- **Uso de UDFs e Window Functions:** Aplica√ß√µes pr√°ticas para processamento avan√ßado.
- **Performance:** Estrat√©gias de particionamento e Broadcast Joins para otimiza√ß√£o.
- **Integra√ß√£o:** Pronto para ler e escrever em Hadoop HDFS e formatos otimizados como Parquet.
- **Documenta√ß√£o:** Cada etapa do processo foi comentada para facilitar o entendimento.

---

##  Melhorias Futuras

- Automatizar testes unit√°rios em PySpark (ex: `pytest + chispa`).
- Implementar logging estruturado (com `log4j` ou `logging` Python).
- Integrar orquestra√ß√£o de tarefas com **Apache Airflow** ou **Databricks Workflows**.

---

## Contato

Caso tenha interesse em discutir mais sobre este projeto:

- **Lindberg Gualberto Ferreira**
- üìß Email: lindberggf@gmail.com
- üíº LinkedIn: [(https://www.linkedin.com/in/lindberg-gualberto-ferreira-57a80078)](https://www.linkedin.com/in/lindberg-gualberto-ferreira-57a80078)

---

> Projeto desenvolvido com dedica√ß√£o e foco em boas pr√°ticas de engenharia de dados.
